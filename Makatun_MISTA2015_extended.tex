% For MISTA 2015, use the default option that has been supplied
\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallextended]{svjour3}     % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}         % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{listings}
\usepackage{colortbl}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% This is preset for MISTA 2015: Do not change
\journalname{MISTA 2015}
%


\begin{document}

\title{Model for planning of distributed data production}

\author{Dzmitry Makatun         \and
		J\'er\^ome~Lauret		\and
		Hana~Rudov\'a			\and
		Michal~\v{S}umbera	
}

\institute{Dzmitry Makatun \at
              Faculty of Nuclear Physics and Physical Engineering, Czech Technical University in Prague \\
              \email{makatun@rcf.rhic.bnl.gov}           %  \\
           \and
           J\'er\^ome~Lauret \at
              STAR, Brookhaven National Laboratory (BNL), USA \\
              \email{jlauret@bnl.gov}          %  \\              
           \and
           Hana~Rudov\'a \at
              Faculty of Informatics, Masaryk University, Brno, Czech Republic \\
              \email{hanka@fi.muni.cz}           %  \\              
           \and
           Michal~\v{S}umbera \at
              Nuclear Physics Institute (NPI), Academy of Sciences (ASCR),
 Czech Republic \\
              \email{sumbera@ujf.cas.cz}           %  \\
}

\maketitle
\section{Introduction}
\label{intro}
The STAR experiment at the Relativistic Heavy Ion Collider (RHIC) studies a
primordial form of matter that existed in the universe shortly after the Big
Bang. Collisions of heavy ions occur millions of times per second inside the
detector, producing tens of petabytes of raw data each year. All the raw data
has to be processed in order to reconstruct physical events which are
further analyzed by scientists. This process is called data production.  Like
any other modern experiment in High Energy and Nuclear Physics (HENP), STAR intends to rely on
distributed data processing, making use of several remote computational sites
(for some experiments this number can scale up to several hundreds).

When running data intensive applications on distributed computational
resources long I/O overheads may be observed as access to remotely stored data
is performed. Latency and bandwidth can become the major limiting factors for
the overall computation performance and can reduce the CPU time\,/\,wall time 
ratio due to excessive I/O wait. 
Widely used data management systems in the HENP community
(Xrootd, DPM) are focused on providing heterogeneous access to distributed
storage and do not consider data pre-placement with respect to available CPUs,
job duration or network performance. At the same time job scheduling systems
(PBS, Condor) do not reason about transfer overheads when accessing data at
distributed storage. For this reason, an optimization of data transferring and
distribution across multiple sites is often done manually, using a custom
setup for each particular infrastructure \cite{Balewski}. 

In previous collaboration between BNL and NPI/ASCR, the problem of
efficient data transferring in a Grid environment was addressed \cite{Zerola}.
% and cache management \cite{Makatun_cache}. 
Data transfers between $n$~computational sites and $m$~data locations were
considered but job scheduling was not covered
by that work. In \cite{ACAT_cp} we
proposed a constraint programming planner that schedules computational jobs
and data transfers in a distributed environment in order to optimize resource
utilization and reduce the overall completion time. Since such global
scheduling is computationally demanding it should be divided into several
stages in order to improve scheduler performance and scalability. A planning of
resource load can be completed in the first stage before scheduling file
transfers and jobs. In this work we address the problem of data production
planning, answering the question how the data should be transferred given the
network structure, bandwidth, storage and CPU slots available. This will allow
local schedulers to process jobs and have CPUs busy all the time while not
exceeding disk and network capacities.

Optimization of data intensive applications in Grid was studied
in~\cite{Globus_scheduler}. In this work an optimization was achieved by
replication of highly used files to more sites while the jobs were executed
where their input data is located. However, this is not the case for data
production, when each file has to be processed once. 
%
Explicit model distributing jobs over a Grid with respect to the network
bandwidth was proposed in~\cite{Trees}. The network structure of the Grid was
modeled as a tree and all the files were assumed to be of the same size and
processing time. In our study we do not limit the network topology to trees,
and assume fluctuations of job parameters. 

\section{Modeling}
\label{modeling}
Due to a data level of parallelism a typical workflow of HENP computation
consists of independent jobs using one CPU, one input and one output file. We
assume there is a local scheduler running at each site, which picks a new
input file to process from the local storage of that site each time when a CPU becomes
free. Input data must be transferred from the central storage
to each site in such a manner that at the every moment of time there is enough
input files at each site to keep all the available CPUs busy while not
exceeding the local storage and network throughput. Another task
is to transfer the output files back to central storage, cleaning each local
storage for the new input.

Let us consider a scheduling time interval $\Delta T$. We assume that at the
starting moment all the CPUs in the Grid are busy, and there is some amount of
input data already placed at each site. We need to transfer the next portion
of data to each site during time interval $\Delta T$ in order to avoid
draining of the local queue by the end of this interval. 

The computational Grid is represented by a directed weighted graph where
vertexes $c_{i} \in C$ are computational nodes and edges $l_{j} \in L$ are
network links. The weight of each link $b_{j}$ is the amount of data that can be
transferred over the link per unit of time (i.e. bandwidth). One of the nodes
$c_{0}$ is the central storage where all the input files for the further
processing are initially placed. All the output files has to be transferred
back to $c_{0}$ from the computational nodes. We will give two separate
problem formulations: for an input and output transfer planning. 

In order to formulate a network flow maximization problem \cite{Network_flows}
for input/output file transferring we have to define a capacitated $\{s,t\}$
network, which is a set of vertexes $V$ including a source $s$ and a sink $t$;
and a set of edges $e\in E$ with their capacities $cap(e)$. A solution that
assigns a non-negative integer number $f(e)$ to each edge $e \in E$ can be
found in polynomial time with known algorithms.

\subsection{Input flow planning}
\begin{figure}[h]
	\begin{center}
		\includegraphics [trim= 30mm 20mm 30mm 30mm , clip, angle =-90, width=0.7\textwidth]{pic/network_general.pdf}
	\end{center}
	\caption{Data production Grid represented as an capacitated $\{s,t\}$ network for input transfer planning (general case). $c_{0}$ is a central NFS storage (Tier-0), $c_{i}$ are computational nodes (where $i>0$), $l_{j}$ are network links, $d_{i}$ are dummy edges from computational nodes to the sink $t$, $q_{0}$ is a dummy edge leading from the source $s$ to the $c_{0}$. }
	\label{network_general}
\end{figure}  
In order to transform a given graph of a Grid into a capacitated $\{s,t\}$
network for an input transfer problem (see Figure \ref{network_general})we add two dummy vertexes: a source $s$
and a sink $t$. Next we add  dummy edges $d_{i} \in D$ from each computational
node $i$ to the sink, and a dummy edge $q_{0}$ from the source $s$ to the
central storage $c_{0}$. These dummy edges allow us to introduce constraints
on the storage capacity of the nodes. The set of vertexes $V$ consists of
computational nodes $C$ and dummy vertexes: $V= C \cup \{s,t\}$. The final set
of edges consists of real network links $L$, dummy edges $D$ from
computational nodes to the sink and from the source to the central storage
$q_{0}$: $E= L \cup D \cup \{q_{0}\}$. Capacity of each edge defines the
maximal amount of data that can be transferred over an edge within time
interval $\Delta T$: 
%
\begin{equation} 
\label{edge_cap} 
cap(e) = \left\{
\begin{array}{l l} 
b_{j} \cdot \Delta T & \text{if }e = l_{j} \in L \\ w_{i} &
\text{if } e = d_{i} \in D\\ k_{0} & \text{if } e = q_{0} 
\end{array} \right.
\end{equation} 
%
where $w_{i}$ is the maximal amount of data that can be
transferred to the node $i$ without exceeding its storage capacity $Disk_{i}$
and $k_{0}$ is the total size of available input files at $c_{0}$. We denote
the solution for the input transfer problem as $f^{in}(e)$.

\subsection{Output flow planning}

\label{outproblem}
\begin{figure}[h]
	\begin{center}
		\includegraphics [trim= 30mm 20mm 30mm 30mm , clip, angle =-90, width=0.7\textwidth]{pic/network_general_out.pdf}
	\end{center}
	\caption{Data production Grid represented as an capacitated $\{t,s\}$ network for output transfer planning (general case). $c_{0}$ is a central NFS storage (Tier-0), $c_{i}$ are computational nodes (where $i>0$), $l_{j}$ are network links, $\overline{d}_{i}$ are dummy edges from the source $t$ to computational nodes, $\overline{q}_{0}$ is a dummy edge leading from $c_{0}$ to the sink $s$. }
	\label{general_out}	
\end{figure}

For transfer of output files we use a similar transformation, but swap the
source $s$ and the sink $t$, change the direction of dummy edges and redefine
capacities of dummy edges (see Figure \ref{general_out}). In this case the capacity $\overline{k}_{0}$ of the
dummy edge $\overline{q}_{0}$ leading from the central storage $c_0$ to the
sink $s$ is equal to the amount of data which can be transferred to $c_0$
within time interval $\Delta T$ (it is limited by the available space at the
central storage). The capacity $\overline{w}_{i}$ of dummy edges
$\overline{d}_{i}$ leading from the source $t$ to computational nodes $c_{i}$
is equal to the maximum amount of output data which can be transferred from
the node $c_{i}$.
%
\begin{equation}
\label{edge_cap_out}
cap(e) = \left\{ 
  \begin{array}{l l}
    b_{j} \cdot \Delta T & \text{if }e = l_{j} \in L \\
    \overline{w}_{i} & \text{if } e = \overline{d}_{i} \in \overline{D}\\
    \overline{k}_{0} & \text{if } e = \overline{q}_{0}
  \end{array} \right.
\end{equation}
%
We denote the solution for the output transfer problem as $f^{out}(e)$.

\subsection{Capacities of dummy edges}
Let us consider data production jobs which perform the same type of processing
on the same type of files. Duration $p_{j}$ of a job $j$  processing an input
file of size $InSize_{j}$ at a node $i$ is $p_{j} = \alpha_{i} \cdot
InSize_{j}$ where $\alpha_{i}$ is constant for each node $i$.  The ratio of
size of input $InSize_{j}$ and output $OutSize_{j}$ files of each job $j$ is
considered to be constant for the same type of data processing, i.e.,
$OutSize_{j} = \beta \cdot InSize_{j}$.  During the time interval $\Delta T$ a
node $i$ with $NCPU_{i}$  of CPUs  will process $\frac{1}{\alpha_{i}} \cdot
NCPU_{i} \cdot \Delta T$ of input data and will produce
$\frac{\beta}{\alpha_{i}} \cdot NCPU_{i} \cdot \Delta T$ of output data.
Using constraints on storage space, we can define the maximal amount of input
$w_{i}$ and output $\overline{w}_{i}$ data which can be transferred to/from a
node $i$:
%
\begin{eqnarray}
w_{i} &=&
Disk_{i} - I_{i}^{in} - I_{i}^{out} + \frac{1 - \beta}{\alpha_{i}} \cdot
NCPU_{i} \cdot \Delta T + Del_{i}^{out} \label{w}\\
\overline{w}_{i} &=& I_{i}^{out} + \frac{\beta}{\alpha_{i}} \cdot NCPU_{i} \cdot \Delta T - Min_{i}^{out} \label{sigma}
\end{eqnarray}  
%
where $Disk_{i}$ is available disk space at the node $i$, $I_{i}^{in}$ and
$I_{i}^{out}$ are the initial size of input and output data at a local storage
respectively, $Del_{i}^{out}$ is the amount of output data that will be
transferred out of the node and deleted from its storage during $\Delta T$;
$Min_{i}^{out}$ is the total size of output files which cannot be transferred
because the jobs which produce them are not finished (output files of running
jobs). 

In the Eqn.~\ref{w}, $Del_{i}^{out}$ is equal to the amount of data which will
be transferred from a node $c_{i}$, i.e., the solution to the output transfer
problem $f^{out}(\overline{d}_{i})$. In the Eqn.~\ref{sigma}, $\Delta T$ and
$Min_{i}^{out}$ are parameters of the scheduler. The other values used in
Eqns.~\ref{w}--\ref{sigma} can be obtained from monitoring data right before
each planning iteration.

\section{Solving Procedure}
\label{solve}

It can be proven that the maximum flow problems for input and output transfers
can be solved independently under assumptions: (a) all the real network links
in the considered Grid are full-duplex, i.e., a network throughput between two
nodes is the same in both directions (b) in a steady state the size of the
output transferred from each node is proportional to the size of the input
transferred to that node in each scheduling interval, i.e.,
$f^{out}(\overline{d}_{i})= \beta \cdot f^{in}(d_{i})$, where $\beta \leq 1$.

Since in real environment the assumption (b) will not strongly hold due to
resource performance fluctuations we propose the following approach to
solve the problem:
%
\begin{enumerate}
\item Calculate values for $\overline{w}_{i}$ using Eqn.~\ref{sigma}.
\item Solve the problem for output data flows to obtain $f^{out}(e)$.
\item Using Eqn.~\ref{w} and $Del_{i}^{out} = f^{out}(\overline{d}_{i})$ calculate $w_{i}$.
\item For real links $l \in L$ reduce the capacity by the amount which is used by output transfers: $cap(l_{j}) = b_{j} \cdot \Delta T - f^{out}(l_{j})$.
\item Solve the problem for input transfers with $w_{i}$ and $cap(l_{j})$ defined in previous steps. Find input data flows $f^{in}(e)$.
\end{enumerate}
%
To conclude, this procedure is expected to compute feasible data transfers 
such that CPUs in Grid are busy with computational jobs while not exceeding 
local disk capacities. An evaluation of the proposed heuristic with the help of Grid simulations is planned as the next part of the research. 

\section{Plan execution}
\label{plan-execution}
. I have implemented a network flow based planner as described in our MISTA paper (see the paper and presentation attached). It works iteratively: collects the data about Grid nodes status, produces the plan for the next time window (which is how much input/output data should be transferred over each link), and sends the plan to all computational nodes.
4. Each computational node has it's disk files and CPUs. I also implemented a "Handler" with is kind of a local scheduler at each node. It is responsible for the plan execution. It receives the plan, submits jobs for execution and transfers input/output files to the neighboring nodes according to the global plan produced by the central planer. It also provides status information to the central planer and collects statistics.
\begin{lstlisting}
//metacode here
\end{lstlisting}

\section{Simulations}
\subsection{Simulation setup}
In our experiments the "Grid Simulation Toolkit For Resource Modelling And Application Scheduling For Parallel And Distributed Computing" (GridSim) \cite{GridSim} was used for realistic simulations of distributed data production. It is a discrete-event simulation toolkit which provides extensive models of computational clusters, job scheduling and execution, network, data transferring and etc. Additional functionality for global plan execution \ref{plan-execution}, statistics collection and a simple storage management was implemented on top of the GridSim package for our simulations.

Sending files over the network is an essential part of our simulations, while it is briefly describe below, the detailed description can be found in \cite{GridSimNetwork}. When a simulated entity (i.e. computational node) executes "transfer file" command, the file is stored to the output queue and then it is processed by underlying network model. If there are several networks links connected to the entity, then each of them has its own queue. Two file transfer modes were used:
\begin{itemize}
\item \textbf{Sequential:} files are transferred one by one in the order as they appear in the queue, only one transfer at a time is performed. 
\item \textbf{Parallel:} all the files in queue are being transferred simultaneously, sharing the bandwidth.  In particular, newly started transfers delay those in progress.
\end{itemize}
An important consequence of the difference between the two modes appears when an entity sends a set of multiple files to the same destination simultaneously. In both modes the complete transfer time of a file set will be the same, but in the \textbf{sequential} transfer mode the files will arrive one by one and the first file will become available faster than in the \textbf{parallel} transfer mode, where the delay of the first file will depend on size of the set. A comparisonal study of parallel / sequential transfer modes in real network can be found in \cite{Zerola}, where the authors has shown that transferring files sequentially (but using multiple threads) can be advantageous for HENP computations compared to parallel transfer of multiple files. However, due to various aspects, the parallel transfer mode is a more common model for distributed data processing in HENP.

In order to test our planning model against other approaches, we have simulated distributed data production under the scheduling scenarios listed below. In all the scenarios, the input data is initially placed at the central storage and all the output data has to be transferred back to it, all the CPUs in the system are free at the initial moment of time.
\begin{itemize}
\item \textbf{PLANER:}  This scenario uses the global planing described in our model. The \textit{sequential transfer network} mode is used as the preferable one. 
\item \textbf{PUSH\_par:} Whenever there is a free CPU in the Grid, the central storage sends next input file to the computational node with this free CPU. When a node receives an input file, it starts processing, and after it is finished it sends the output file back to the central storage. When the central storage receives an output file it is informed that a CPU became free at the sender node. In such a manner, the central storage keeps sending input files until all the data are processed. The shortest network path is used for file transfers. The \textit{parallel transfer network} mode is used here, which leads to uncoordinated concurrent access to remote files by simultaneous jobs. This scenario is the closest to the distributed data production setup in many HENP experiments, in particular, data production at KISTI for the STAR experiment \cite{KISTI-production}. 
\item \textbf{PUSH\_seq:} The job scheduling in performed exactly as in the previous scenario, but the \textit{sequential transfer network} mode is used. The main purpose of these simulations is to study the effects of sequential file transferring on the data production and, also, to estimate which part of the performance improvement in the PLANER scenario is achieved by the sequential transferring alone.
\item \textbf{no\_network:} Again, the job scheduling is the same as in two previous scenarios, but the network is not considered here. The production is performed as if all the CPUs are aggregated at the single site and can read/write the data directly from/to the storage with no latency. This scenario serves as a base case for comparison. It allows to estimate the limit for the processing makespan, and estimate the influence of the transfer latency. Obviously, none of the other scenarios can process the data faster than the one with no network delay.
\end{itemize}


The main metrics used for performance comparison is the makespan, which is calculated as time passed from the start of the first input file transfer from the central storage until the finish time of the last output file transfer to it. For comparison of two scheduling policies, a makespan improvement can be calculated as follows:


\begin{equation}
\label{makespanImprovement}
 makespan~improvement = \frac{makespan_{1} - makespan_{2}}{makespan_{1}} 
\end{equation}

In our simulations we reuse the network models provided by the GridSim package \cite{GridSimNetwork}. Each simulated entity (i.e. computational node) has it's input/output ports for sending/receiving IO data. When an entity executes the command "send the data to the destination" the data is directed to the entity's output port and placed to the port's queue. Then it is processed by the selected underlying network mechanism. 

The sequential transfer model uses the default network model of the GridSim ("NET\_PACKET\_LEVEL"). At each entity (nodes and routers), there is a FIFO scheduler, which takes an event (data) from the entity's output port and schedules it's transfer. The end of transfer is calculated by the network propagation delay i.e. data size / bandwidth of the selected link. At the end time of transfer the data becomes available at the destination (input port of the node) and the FIFO scheduler processes the next data from the output queue.

The parallel transfer model uses the flow network model of the GridSim ("NET\_FLOW\_LEVEL"). Here, when the FIFO scheduler process the data from the output port, it creates a flow at the selected link. If several flows exist on the link, they share equal fractions of the bandwidth. Then, based on the allocated flow bandwidth, the end time of the transfer in calculated. BUT, if the next data appears  in the output port queue before the current transfers are finished, then the scheduler allocates another flow at the link and recalculates bandwidth of already existing flows, and then it also recalculates the end times of ongoing transfers as remaining data size / new allocated bandwidth.

I think this model should be adequate for the proof of concept. More details on the network model are at the articlehttp://www.buyya.com/papers/gridsimedu.pdf. However, it seems like many things were added since it was published.  More data about GridSim is there http://www.buyya.com/gridsim/. Of cause the background trafic effects can be also studied using the GridSim, but this will require a lot of additional time. But I think this could be done for the next publications. 

\subsection{Using real log data}
Descriprion of of dataset, data production at KISTI,
The values for network bandwidth and number of CPUs at computational nodes were set to be comparable with data available in ALICE Grid monitoring system \cite{MonAlisa}

5. The jobs are taken from the KISTI log. We use input/output file size and duration of the jobs.


\begin{table}
\caption{Parameters of data production jobs used for simulation}
\label{tab:jobs}
\begin{center}
\begin{tabular}{ l  c  l  l  l  }
\hline \hline
% \br                   
    & Units& Average & Min & Max \\ \hline %\mr 
  Duration & s & & & \\ 
  Input file size & MB & & & \\	
  Output file size & MB & & & \\
  $\alpha$ & s / MB & & & \\
  $\beta$ & - & & & \\	
  \hline \hline
 % \br 
\end{tabular}
    \end{center}
\end{table}

\subsection{Simulations with one remote node}
6.  As one of the testing examples I created an environment with 2 nodes: RCF - where all the input files are initially placed and output has to be delivered, but not processing is going on there, and a KISTI site with 1000 CPUs, ~ 15 TB storage and 1 Gbps connection to RCF. The dataset consists of 7158 input files (~25 TB). The simulation runs for about 15 minutes. The simulated makespan of this data production (including unsaturated periods at the beginning and the end) appears 11 days 04:54:23. You can see the monitoring plots about the disk, CPU and network utilization attached. 

KISTI site with 1000 CPUs connected to the central storage RCF (0 CPUs) over a link. The link bandwidth was changing within 0.1 - 2 Gbps. The storage can be considered as infinite for this simulations (we study the effects of network slowdown). The dataset consisted of 7158 jobs  (total input 25 TB, total output 1.4 TB)

A single remote site with 1000 CPUs and 12 TB disk is connected to the central storage. I have run a set of simulations, with different network bandwidth (each dot on the plot is a separate simulation with 7158 jobs). The plot shows the makespan improvement of PLANER against PUSH depending on the network bandwidth. In the best case the improvement can reach 26 \% which, I assume, is good. For the case which is closer to real life (1 Gbps) the improvement is almost 4\%.

\begin{figure}[h]
\begin{minipage}{.49\textwidth}
\centering
    \includegraphics[trim =00mm 00mm 00mm 00mm ,clip, width=1\textwidth]{pic/1node_relative_makespan.pdf}

    \caption{Dependence of makespan on network bandwidth for one remote node with 1000 CPUs}
    \label{fig:simulated grig}
\end{minipage}\hspace{1pc}%
\begin{minipage}{.49\textwidth}
\centering
    \includegraphics[trim =00mm 00mm 00mm 00mm ,clip, width=1\textwidth]{pic/1node_makespan_improvement.pdf}
    \caption{Makespan improvement of proposed model compared to PUSH par and PUSH seq}
    \label{fig:path}
\end{minipage} 
\end{figure}
The conclusions for the simulations with just one remote node are the following:

- The model works well, it manages to transfer files in advance and balance the load.
- BUT in the realistic setup (1 Gbps) the gained improvement is just 0.2 \%. 
-If we remove the network delay completely we will gain 2.3\% of the overall makespan (for 1 Gbps). That's because the job transfer time (~5000 seconds) makes ~2.5 \% of job duration (200 000 seconds).
-when the bandwith is smaller then 0.2 Gbps it is not enough to saturate the CPUs at KISTI and both policies behave the same.
-when the bandwidth is larger then 0.7 Gbps, there are no transfer collisions for the PUSH model. and IO latency is very small comparing to job duration, thus it makes almost no difference either to transfer files in advance or on the go. 


I did the simulations with different numbers of CPUs. In this simulations there was 1 remote site connected to the central storage with 1 Gbps link, and the number of CPUs was changing from 1000 to 6000. The size of storage was adjusted to the number of CPUs (10 TB for every 1000 CPUs).
Again 3 strategies were compared:  PLANER, PUSH\_par (parallel transfer mode) and PUSH\_seq (sequential transfer mode.) + simulations with no network delay (no\_network). 
The results of simulations are shown in the attached plots. 


\begin{figure}[h]
\begin{minipage}{.49\textwidth}
\centering
    \includegraphics[trim =00mm 00mm 00mm 00mm ,clip, width=1\textwidth]{pic/relative_makespan_vs_cpu.pdf}

    \caption{Dependence of makespan on number of CPUs at the remote node connected over 1 Gbps link.}
    \label{fig:simulated grig}
\end{minipage}\hspace{1pc}%
\begin{minipage}{.49\textwidth}
\centering
    \includegraphics[trim =00mm 00mm 00mm 00mm ,clip, width=1\textwidth]{pic/makespan_improvement_vs_cpu.pdf}
    \caption{Makespan improvement of proposed model compared to PUSH par and PUSH seq}
    \label{fig:path}
\end{minipage} 
\end{figure}
- when number of CPUs is $<1000$, then the 1 Gbps link is enough to saturate all the CPUs without any significant transfer latency. For this reason all 3 algorithms show the same makespan which is very close to the no\_network case.
-when the number of CPUs is $>6000$, the performance is limited by the network bandwidth (1 Gbps is far to low) and adding additional CPUs doesn't improve makespan.
-The planer can deliver up to 14\% makespan improvement compared to PUSH\_par, and 1,8\% compared to PUSH\_seq. Which proves that in the case of a single remote node the main part of the makespan improvement is due to sequential transferring, rather then transferring files in advance. 
- Don't be confused by the low improvement compared to PUSH\_seq (those 1,8 \%). Firstly, the real-life scenario is PUSH\_par (uncoordinated concurrent access to remote files by simultaneous jobs). Secondly, the planer is supposed to shine in more complex network topologies. The case of just 1 remote node serves the purpose to test the planer and to better understand the process. 




\subsection{Simulations with complex Grid structure}

In this simulation 3 computational sites with 1000 CPUs and 12 TB disk each are connected as it is shown at the picture 
grid.pdf. The FAST / MEDIUM / SLOW sites have 1 / 0.3 / 0.1 Gbps connections to the central storage respectively. Also, all the remote sites are interconnected by 0.1 Gbps links. In this case the PUSH model is able to use only the direct connections from the central storage to the comp. sites, while the PLANER uses the interconnection between them in addition. The plot shows the CPU utilization (fraction of computing CPUs over total CPUs in the system) as a function of time. It can be easily seen, that the PUSH model can't utilize 100\% of CPUs, and the number of busy CPUs is fluctuating over time. But the PLANER achieves 100\% utilization very fast and keeps it until the end of simulation. The makespan improvement in this simulation is 28 \%.


I have a series of simulations with larger grid: including 3 computational nodes, 1 central storage, direct + interconnecting links. In the simulations I compared PLANER to both PUSH\_par (parallel transfer mode) and PUSH\_seq (sequential transfer mode.)

The network structure is as before (see picture grid.pdf). But this time the bandwidth of interconnecting links (links between remote nodes = perimeter links at the picture) was changing from 0.01 to 0.5 Gbps. 
The plot 3nodes\_1storage\_makespan.pdf shows makespans of 3 models + makespan without network delay.
The plot 3nodes\_1storage\_makespan\_improvement.png shows makespan improvement of planer compared to PUSH\_par (up to 28\%) and PUSH\_seq (up to 21\%).

The plot 3models\_link01.png gives an example of CPU utilization of all 3 models with 0.1 Gbps interconnecting links.

The main conclusion: the planner can successfully  utilize available network capacity in order to "preplace" data for computations. And this can lead to a significant performance improvement comparing to the traditional scenario, when data is transferred on the go. 

\begin{figure}[h]
  \begin{center}
    \includegraphics [trim= 0mm 80mm 0mm 30mm , clip,       	width=0.5\textwidth]{pic/grid.pdf}
    \caption{Simulated Grid.}
  \end{center}  
  \label{simulated_grid}	
\end{figure}

\begin{figure}[h]
  \begin{center}
    \includegraphics [trim= 0mm 00mm 0mm 00mm , clip,       	width=0.7\textwidth]{pic/3models_link01.png}
    \caption{Comparison of CPU usage of three models. }
  \end{center}  
  \label{multi_cpu_consumption}	
\end{figure}

\begin{figure}[h]
  \begin{center}
    \includegraphics [trim= 0mm 00mm 0mm 00mm , clip,       	width=0.8\textwidth]{pic/3nodes_1storage_makespan_improvement.pdf}
    \caption{Makespan improvement of proposed model compared to PUSH par and PUSH seq}
  \end{center}  
  \label{multi_makespan_compare}	
\end{figure}


\section{Conclusion}
\label{Conclusion}

In this paper we proposed a model of distributed data production, where all
the files from a single source has to be processed once and transferred back.
This model allows planning of WAN, storage and CPU loads using the network
flow maximization approach. The proposed model will be used in a distributed
data production planner which is being developed. The planner will enable
automated and scalable planning and optimization of distributed computations
which are highly required in data intensive computational fields such as High Energy and Nuclear Physics.

\begin{acknowledgements}
This work has been supported by the Czech Science Foundation
(13-20841S, P202$/$12$/$0306),  the MEYS grant CZ.1.07/2.3.00/20.0207 of the European Social Fund (ESF) in the Czech Republic: “Education for Competitiveness Operational Programme” (ECOP) and the Office of Nuclear Physics within the U.S. Department of Energy.  
\end{acknowledgements}

\bibliography{bibliography}{}
\bibliographystyle{spmpsci}

\end{document}

\end{document}
